{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0404e486-b27a-49f0-928d-0e31df74b99e",
   "metadata": {},
   "source": [
    "# Recitation 7: Finetune LM using HuggingFace\n",
    "_Date_: 11/6/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f7615-c580-4923-9e36-b26363bc9e13",
   "metadata": {},
   "source": [
    "## References\n",
    "- [HuggingFace NLP example notebooks](https://huggingface.co/docs/transformers/en/notebooks)\n",
    "- [NLP course](https://huggingface.co/learn/llm-course/en/chapter3/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911893bf-6b30-4ff1-90e3-a088766d3da5",
   "metadata": {},
   "source": [
    "## How to use HuggingFace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b404d1-321f-4d29-bc7e-2bbadbdfca7a",
   "metadata": {},
   "source": [
    "Recall the pipeline training a deep neural model using PyTorch,\n",
    "1. Data representation\n",
    "2. Build model\n",
    "3. Train the model on training/validation set\n",
    "4. Evaluate the model\n",
    "5. Save/publish the model\n",
    "\n",
    "HuggingFace provides APIs for each step, so accomplishing the whole pipeline becomes easy if users don't consider customize the model and have enough computing resources. The library `transformers` is the main one to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf04dec-12f7-458c-95b2-bd817e304a0c",
   "metadata": {},
   "source": [
    "## Task: Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924f8e7-cd1b-4a2a-a2cf-c47f0667ee99",
   "metadata": {},
   "source": [
    "- Arch: Masked language models\n",
    "- Model: BERT\n",
    "- Dataset: GLUE (General Language Understanding Evaluation benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6d3c7-9ec4-4518-a1cd-16a3149cee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf586d-62df-437c-bbca-fd8236762225",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model: str\n",
    "    batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d63740-10a1-48f2-9ec0-0ce4ec073852",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config(model=\"bert-base-uncased\", batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600cde6-fce5-4607-afa3-7263085d0423",
   "metadata": {},
   "source": [
    "### 1) Data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fac3d0-151b-4da8-b62b-b3c052c5363d",
   "metadata": {},
   "source": [
    "This is the first but the most flexible step among the whole pipeline, as users have different dataset to clean, preprocess and represent. Therefore, this step would normally cost you the longest time.\n",
    "\n",
    "Normally, you would need APIs from two libraries:\n",
    "- `datasets` [(link)](https://huggingface.co/docs/datasets/index)\n",
    "  - Load datasets uploaded by people in HuggingFace community\n",
    "  - Similar to `Dataset` class, the library provides a class wrapping up your customized dataset for downstream trainer to process.\n",
    "- `transformers` [(link)](https://huggingface.co/docs/transformers/en/index)\n",
    "  - The main library for building training/inference pipeline which mainly includes **model** and **tokenizer**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e966ec2-652e-4103-b621-9694cbefd634",
   "metadata": {},
   "source": [
    "#### 1a) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e5662-3728-425a-9c0b-1355d4cdaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e661ca-d22c-45f0-add8-fb44422f58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_ds = load_dataset(\"glue\", \"mrpc\")\n",
    "glue_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f5501-a07a-4c5c-ba93-482af923ca09",
   "metadata": {},
   "source": [
    "As you can see, similar to PyTorch, HuggingFace also uses map-style dataset for storing and retrieving raw data. Of course, when the size of data is enormously large, an iterable-style would be utilized.\n",
    "\n",
    "To better understand this map-style dataset, we can see it as a tabular sheet:\n",
    "|sentence1      |sentence2     |label |idx |\n",
    "|---------------|--------------|------|----|\n",
    "|Pizza is great.| You're right.|1     |10  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e507a-283d-45c1-a3e9-931eb7bc0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(glue_ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030a576-1928-4dff-8c68-34dfb32cb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(glue_ds['train'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc6e9f-25d4-4f8f-b923-8df9d9e70130",
   "metadata": {},
   "source": [
    "#### 1b) Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac5022-e060-472b-b77a-c43feb8f6e9f",
   "metadata": {},
   "source": [
    "Now given raw textual data, the next step is to preprocess them, that is, representing linguistic features and encoding them into numerical values. In `transformers`, the tokenizer will be responisble for this step and each model has its own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abe4c7-3bd2-4bb9-ad2b-ef4dff921458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c02018-3a44-4313-a2a3-a3514753584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(conf.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfc989-a476-467c-a55d-e205ca65d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84759e-f3b6-4d0d-a40a-4cb2a4c75bff",
   "metadata": {},
   "source": [
    "_**NOTE**_: `token_type_ids` is an optional field and indicates tokens are from $(i+1)^{th}$ sentence. Here, `0` stands for token is from sentence $1$ and `1` stands for token is from sentence $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fffa1-d3e0-4733-87d2-78d34da4e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(example['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773abcd-854c-4f1f-a2f9-c04e2c8c9758",
   "metadata": {},
   "source": [
    "_**[NOTE]**_: the tokenizer merges two sentences into one along with special tokens `[CLS], [SEP]` specifically for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cf852-7e00-436f-ae65-22143ff6b673",
   "metadata": {},
   "source": [
    "Previous example shows how tokenizer works for a single example, but the training loop processes a batch of examples in practice.\n",
    "\n",
    "`Dataset.map(func, batched=True)` function is helpful for accomplishing this task. You can understand this function applies the `func` as a collate function to every instance in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba099fe-b3b4-4af7-87be-ba708c276b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess in batch\n",
    "def tokenize_fn(instance):\n",
    "    return tokenizer(instance['sentence1'], instance['sentence2'], truncation=True)\n",
    "    \n",
    "tokenized_ds = glue_ds.map(tokenize_fn, batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4d235-5aad-4189-87be-d12f2ec2de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3bc0b-1d3c-45bb-be40-606e0796aa4a",
   "metadata": {},
   "source": [
    "### 2) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c09956-3617-49c9-a592-a169d963f092",
   "metadata": {},
   "source": [
    "Once having dataset preprocessed, the next step is to train an existing model by this specific dataset (called fine-tuning).\n",
    "\n",
    "In `transformers`, you normally need three components:\n",
    "- `TrainingArguments` ([doc](https://huggingface.co/docs/transformers/en/main_classes/trainer)): an object for specifying arguments for training\n",
    "- `Trainer`: the driver to achieve training customized by `TrainingArguments`\n",
    "- `AutoModel[...]` ([doc](https://huggingface.co/docs/transformers/en/model_doc/auto)): the model architecture object to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa6d09-96de-486e-a078-9880f05980b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749d56d-ee6e-4559-91db-4264b24b0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "# 2. Instantiate model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(conf.model, num_labels=2)\n",
    "\n",
    "# 3. Instantiate trainer and start training\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a2daf-4a1f-4721-906a-65391576fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

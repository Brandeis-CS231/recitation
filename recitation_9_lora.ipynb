{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb4af6a-8456-46ae-ac8c-e8a177da82ec",
   "metadata": {},
   "source": [
    "# Recitation 9: Low-Rank Adaptation\n",
    "_Date_: 12/4/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57356361-cb7a-4157-804b-41dc40602902",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2328c-9373-4c3d-aacd-bb8043e7d10d",
   "metadata": {},
   "source": [
    "### Parameter-Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1067542-9e0a-4187-b56f-09abe815f713",
   "metadata": {},
   "source": [
    "As LLMs normally have many parameters, it's very computationally expensive to \"tune\" each parameter to fit a new set of observations (data). \n",
    "\n",
    "(Imagine a lego built with thousands of pieces supports various lookings, you would be crazy to disassemble all pieces and reassemble them together to get a new look)\n",
    "\n",
    "**Question**: Is it possible to have a magical glue/adaptor plugged in this giant \"machine\" so that we can only tweak very few amount of \"pieces\"?\n",
    "\n",
    "**Answer**: Yes, PEFT\n",
    "\n",
    "**Question**: By how? Or where do we place the adaptor in a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7958776-6bf2-4552-829b-9f05fc8fb660",
   "metadata": {},
   "source": [
    "### Rank (of a matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bb04e-7538-4668-b8b5-12693388851c",
   "metadata": {},
   "source": [
    "Rank have two kinds: column rank and row rank\n",
    "\n",
    "_**Definition**_ (Column rank): the number of linearly independent column vectors of a matrix\n",
    "\n",
    "_**Definition**_ (Linear independence): Given a matrix represented as column vectors $M = [v_1, v_2, \\cdots, v_n]$, an linearly dependent column vector $v_j$ if it is a linear combination of rest of column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5badbff3-ef47-4977-981c-3da4759e1edd",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dbf6af-85a2-4265-9ca9-6b77f5692383",
   "metadata": {},
   "source": [
    "LoRA introduces an additional weight matrix $\\Delta W = B \\times A$ to a pre-trained model with weight $W$, where $A \\in \\mathbb{R}^{r \\times d}, B \\in \\mathbb{R}^{d \\times r}$ are low-rank matrices and $r \\ll d$.\n",
    "\n",
    "LoRA is often applied to the tranformation for $Q$ and $V$, so $Q = X \\cdot (W_Q + \\Delta W)$ and $V = X \\cdot (W_V + \\Delta W)$.\n",
    "\n",
    "_**Question**_: Why does LoRA apply to the layer computing self-attention?\n",
    "\n",
    "_**Question**_: What is the hyperparameter for LoRA?\n",
    "\n",
    "> NOTE: Other than applying to the process computing $Q, V$, it's worth trying to adapt LoRA to FFN or key $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dac30d-32ed-4367-8f36-c454be16ff13",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524d8f4-8b1d-4e47-a0d4-40088cfd8477",
   "metadata": {},
   "source": [
    "Other than instaniating and loading a model using the library `transformers`, loading a LoRA adaptor needs another library `peft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb934b-e322-49ad-bacb-5852df686629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d2b71-4ddf-43fe-a9fa-1de12c7bbdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a53219-c1c8-41ee-b760-62fa5f1042f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a4d58-72a2-4d55-8464-6b6c47459252",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0775c-8cd5-49d2-b0b9-691b267231c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c3107-598f-4333-9f49-7d92f6fb979d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
